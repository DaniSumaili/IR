{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import unicodedata\n",
    "import re\n",
    "import time \n",
    "from datetime import datetime, timedelta\n",
    "from threading import Timer\n",
    "\n",
    "#Schedule run every 7 days at 1am \n",
    "\n",
    "x=datetime.today()\n",
    "y = x.replace(day=x.day, hour=1, minute=0, second=0, microsecond=0) + timedelta(days=7)\n",
    "delta_t=y-x\n",
    "secs=delta_t.total_seconds()\n",
    "\n",
    "def research():\n",
    "    stflinks = []\n",
    "    stfname = []\n",
    "    name = []\n",
    "    coven = pd.DataFrame(columns= ['name', 'link','research_interest'])\n",
    "    page1 = requests.get(\"https://scholar.google.co.uk/citations?view_op=view_org&hl=en&org=9117984065169182779\")\n",
    "    soup1 = BeautifulSoup(page1.text, 'html.parser')\n",
    "    #sub link to a staff page\n",
    "\n",
    "    for main in soup1.find_all('div', class_='gsc_1usr'):\n",
    "        try:\n",
    "            slink = main.h3.a['href']\n",
    "        except Exception as e:\n",
    "            slink = 'None' \n",
    "        stflinks.append(slink)\n",
    "\n",
    "    #staff page extract details(name and research area)   \n",
    "    for stfpage in stflinks:\n",
    "        pagestf= requests.get(\"https://scholar.google.co.uk\" + str(stfpage))\n",
    "        soup2 = BeautifulSoup(pagestf.text, 'html.parser')                      \n",
    "        try:\n",
    "            sname = soup2.find('div', id ='gsc_prf_in').text\n",
    "        except Exception as e:\n",
    "            sname = 'None'  \n",
    "\n",
    "    #all area research in one row         \n",
    "        stfarea = []\n",
    "        for main1 in soup2.find_all(class_='gsc_prf_inta gs_ibl'):\n",
    "            try:\n",
    "                stfar = main1.text\n",
    "            except Exception as e:\n",
    "                stfar = 'None'\n",
    "            #stfarea.append('\\n')    \n",
    "            stfarea.append(stfar)\n",
    "\n",
    "\n",
    "        links = (\"https://scholar.google.co.uk\" + str(stfpage))\n",
    "        string = \", \"\n",
    "        string = string.join(stfarea)\n",
    "    #Dataframe     \n",
    "        a = {'name': sname, 'link' : links, 'research_interest': string}\n",
    "        cov = pd.DataFrame.from_dict(a, orient='index')\n",
    "        coventry = cov.transpose()    \n",
    "        coven = coven.append(coventry, ignore_index= 'True')\n",
    "        coven.to_csv('coventry1.csv')\n",
    "\n",
    "\n",
    "    for main3 in soup1.find_all('div', class_= 'gsc_pgn'):\n",
    "        for main3 in soup1.find_all(class_= 'gs_btnPR gs_in_ib gs_btn_half gs_btn_lsb gs_btn_srt gsc_pgn_pnx'):\n",
    "            try:\n",
    "                nextlink = main3['onclick']            \n",
    "            except Exception as e:\n",
    "                nextlink = 'None' \n",
    "            start_pos = nextlink.find('=')\n",
    "            temp= nextlink[start_pos + 2:len(nextlink) -1]\n",
    "\n",
    "    def normalize_string(text):\n",
    "        \n",
    "        text = text.replace(\"\\\\x3d\", \"=\")    \n",
    "        text = text.replace(\"\\\\x26\", \"&\")    \n",
    "        text = text.strip()\n",
    "\n",
    "        text = text.rstrip('.')\n",
    "        text = unicodedata.normalize(u'NFKD', str(text)).encode('ascii', 'ignore').decode('utf8')\n",
    "\n",
    "        return text \n",
    "    normalize_string(temp)\n",
    "\n",
    "    link = 'https://scholar.google.co.uk' + normalize_string(temp)\n",
    "    #print(link)\n",
    "    \n",
    "    \n",
    "#Using the link to to the next page\n",
    "    i= 1\n",
    "    while i: \n",
    "        if nextlink != 'None':\n",
    "            stflinks = []\n",
    "            stfname = []\n",
    "            name = []\n",
    "            coven = pd.DataFrame(columns= ['name', 'link','research_interest'])\n",
    "            page1 = requests.get(link)\n",
    "            soup1 = BeautifulSoup(page1.text, 'html.parser')\n",
    "            #sub link to a staff page\n",
    "\n",
    "            for main in soup1.find_all('div', class_='gsc_1usr'):\n",
    "                try:\n",
    "                    slink = main.h3.a['href']\n",
    "                except Exception as e:\n",
    "                    slink = 'None' \n",
    "                stflinks.append(slink)\n",
    "\n",
    "            #staff page extract details(name and research area)   \n",
    "            for stfpage in stflinks:\n",
    "                pagestf= requests.get(\"https://scholar.google.co.uk\" + str(stfpage))\n",
    "                soup2 = BeautifulSoup(pagestf.text, 'html.parser')                      \n",
    "                try:\n",
    "                    sname = soup2.find('div', id ='gsc_prf_in').text\n",
    "                except Exception as e:\n",
    "                    sname = 'None'  \n",
    "\n",
    "            #all area research in one row         \n",
    "                stfarea = []\n",
    "                for main1 in soup2.find_all(class_='gsc_prf_inta gs_ibl'):\n",
    "                    try:\n",
    "                        stfar = main1.text\n",
    "                    except Exception as e:\n",
    "                        stfar = 'None'\n",
    "                    #stfarea.append('\\n')\n",
    "                    stfarea.append(stfar)\n",
    "\n",
    "\n",
    "                links = (\"https://scholar.google.co.uk\" + str(stfpage))\n",
    "                string = \", \"\n",
    "                string = string.join(stfarea)\n",
    "            #Dataframe     \n",
    "                a = {'name': sname, 'link' : links, 'research_interest': string}\n",
    "                cov = pd.DataFrame.from_dict(a, orient='index')\n",
    "                coventry = cov.transpose()    \n",
    "                coven = coven.append(coventry, ignore_index= 'True')\n",
    "            coven.to_csv('coventry1.csv', mode='a', header=False)\n",
    "\n",
    "            #Nextpage link\n",
    "            for main3 in soup1.find_all('div', class_= 'gsc_pgn'):\n",
    "                for main3 in soup1.find_all(class_= 'gs_btnPR gs_in_ib gs_btn_half gs_btn_lsb gs_btn_srt gsc_pgn_pnx'):\n",
    "                    try:\n",
    "                        nextlink = main3['onclick']            \n",
    "                    except Exception as e:\n",
    "                        nextlink = 'None' \n",
    "                    start_pos = nextlink.find('=')\n",
    "                    temp= nextlink[start_pos + 2:len(nextlink) -1]\n",
    "\n",
    "            def normalize_string(text):\n",
    "\n",
    "                text = text.replace(\"\\\\x3d\", \"=\")    \n",
    "                text = text.replace(\"\\\\x26\", \"&\")    \n",
    "                text = text.strip()\n",
    "\n",
    "                text = text.rstrip('.')\n",
    "                text = unicodedata.normalize(u'NFKD', str(text)).encode('ascii', 'ignore').decode('utf8')\n",
    "\n",
    "                return text \n",
    "            normalize_string(temp)\n",
    "\n",
    "            link = 'https://scholar.google.co.uk' + normalize_string(temp)\n",
    "            print(link)\n",
    "        else:\n",
    "            break\n",
    "        #random pause between 1 to 5 sec \n",
    "        sleep(randint(1,5))        \n",
    "        i += 1   \n",
    "        \n",
    "        \n",
    "t = Timer(secs, research)\n",
    "t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Schedule crawling on every 7 days at 1am \n",
    "x=datetime.today()\n",
    "y = x.replace(day=x.day, hour=1, minute=0, second=0, microsecond=0) + timedelta(days=7)\n",
    "delta_t=y-x\n",
    "secs=delta_t.total_seconds()\n",
    "\n",
    "def research():\n",
    "    #the seed url will output a link to be used to go to the next page\n",
    "    i= 1\n",
    "    while i: \n",
    "        if nextlink != 'None':\n",
    "            stflinks = []\n",
    "            stfname = []\n",
    "            name = []\n",
    "            coven = pd.DataFrame(columns= ['name', 'link','research_interest'])\n",
    "            page1 = requests.get(link)\n",
    "            soup1 = BeautifulSoup(page1.text, 'html.parser')                   \n",
    "        else:\n",
    "            break\n",
    "        #random crawler pause between 1 to 5 sec after each page \n",
    "        sleep(randint(1,5))        \n",
    "        i += 1       \n",
    "        \n",
    "t = Timer(secs, research)\n",
    "t.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
