{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Scholar - Coventry University potential reasearch supervisors for postgraduate research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required libraries\n",
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling first page "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the the seed URL to crawl the first page \n",
    "1. Get part of the weblink to crawl individual professor/personnel page\n",
    "2. Combine part of the URL to the main google scholar link\n",
    "3. From this page get the name, research category\n",
    "4. Combine the person informations into dataframe and save it as a CSV file to easy append all pages data\n",
    "5. Get the link to the next page, google scholar have this not normalize using character replacement and\n",
    "    unicode normalization to transform into normal string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=zMpCAD_n__8J&astart=10\n"
     ]
    }
   ],
   "source": [
    "stflinks = []\n",
    "stfname = []\n",
    "name = []\n",
    "coven = pd.DataFrame(columns= ['name', 'link','research_field'])\n",
    "page1 = requests.get(\"https://scholar.google.co.uk/citations?view_op=view_org&hl=en&org=9117984065169182779\")\n",
    "soup1 = BeautifulSoup(page1.text, 'html.parser')\n",
    "#sub link to a staff page\n",
    "\n",
    "for main in soup1.find_all('div', class_='gsc_1usr'):\n",
    "    try:\n",
    "        slink = main.h3.a['href']\n",
    "    except Exception as e:\n",
    "        slink = 'None' \n",
    "    stflinks.append(slink)\n",
    "    \n",
    "#staff page extract details(name and research area)   \n",
    "for stfpage in stflinks:\n",
    "    pagestf= requests.get(\"https://scholar.google.co.uk\" + str(stfpage))\n",
    "    soup2 = BeautifulSoup(pagestf.text, 'html.parser')                      \n",
    "    try:\n",
    "        sname = soup2.find('div', id ='gsc_prf_in').text\n",
    "    except Exception as e:\n",
    "        sname = 'None'  \n",
    "        \n",
    "#all area research in one row         \n",
    "    stfarea = []\n",
    "    for main1 in soup2.find_all(class_='gsc_prf_inta gs_ibl'):\n",
    "        try:\n",
    "            stfar = main1.text\n",
    "        except Exception as e:\n",
    "            stfar = 'None'\n",
    "        #stfarea.append('\\n')    \n",
    "        stfarea.append(stfar)\n",
    "        \n",
    "       \n",
    "    links = (\"https://scholar.google.co.uk\" + str(stfpage))\n",
    "    string = \", \"\n",
    "    string = string.join(stfarea)\n",
    "#Dataframe     \n",
    "    a = {'names': sname, 'link' : links, 'research_field': string}\n",
    "    cov = pd.DataFrame.from_dict(a, orient='index')\n",
    "    coventry = cov.transpose()    \n",
    "    coven = coven.append(coventry, ignore_index= 'True')\n",
    "    coven.to_csv('coventry1.csv')\n",
    "\n",
    "\n",
    "for main3 in soup1.find_all('div', class_= 'gsc_pgn'):\n",
    "    for main3 in soup1.find_all(class_= 'gs_btnPR gs_in_ib gs_btn_half gs_btn_lsb gs_btn_srt gsc_pgn_pnx'):\n",
    "        try:\n",
    "            nextlink = main3['onclick']            \n",
    "        except Exception as e:\n",
    "            nextlink = 'None' \n",
    "        start_pos = nextlink.find('=')\n",
    "        temp= nextlink[start_pos + 2:len(nextlink) -1]\n",
    "        \n",
    "def normalize_string(text):\n",
    "    \n",
    "    text = text.replace(\"\\\\x3d\", \"=\")    \n",
    "    text = text.replace(\"\\\\x26\", \"&\")    \n",
    "    text = text.strip()\n",
    "\n",
    "    text = text.rstrip('.')\n",
    "    text = unicodedata.normalize(u'NFKD', str(text)).encode('ascii', 'ignore').decode('utf8')\n",
    "    \n",
    "    return text \n",
    "normalize_string(temp)\n",
    "\n",
    "link = 'https://scholar.google.co.uk' + normalize_string(temp)\n",
    "print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>link</th>\n",
       "      <th>research_field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Timothy Mason</td>\n",
       "      <td>https://scholar.google.co.uk/citations?hl=en&amp;u...</td>\n",
       "      <td>sonochemistry, ultrasound, chemistry, environm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gurnam Singh</td>\n",
       "      <td>https://scholar.google.co.uk/citations?hl=en&amp;u...</td>\n",
       "      <td>social work, race and racism, critical pedagog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WD Li</td>\n",
       "      <td>https://scholar.google.co.uk/citations?hl=en&amp;u...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dr. Mohammad M Ali</td>\n",
       "      <td>https://scholar.google.co.uk/citations?hl=en&amp;u...</td>\n",
       "      <td>Forecast Information Sharing, ARIMA Modelling,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petra Wark</td>\n",
       "      <td>https://scholar.google.co.uk/citations?hl=en&amp;u...</td>\n",
       "      <td>m/eHealth, epidemiology, primary prevention, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name                                               link  \\\n",
       "0       Timothy Mason  https://scholar.google.co.uk/citations?hl=en&u...   \n",
       "1        Gurnam Singh  https://scholar.google.co.uk/citations?hl=en&u...   \n",
       "2               WD Li  https://scholar.google.co.uk/citations?hl=en&u...   \n",
       "3  Dr. Mohammad M Ali  https://scholar.google.co.uk/citations?hl=en&u...   \n",
       "4          Petra Wark  https://scholar.google.co.uk/citations?hl=en&u...   \n",
       "\n",
       "                                      research_field  \n",
       "0  sonochemistry, ultrasound, chemistry, environm...  \n",
       "1  social work, race and racism, critical pedagog...  \n",
       "2                                                     \n",
       "3  Forecast Information Sharing, ARIMA Modelling,...  \n",
       "4  m/eHealth, epidemiology, primary prevention, d...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coven.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the link above \n",
    "1. Using a while loop to crawl through the rest of the pages\n",
    "2. using a sleep time with random generated time between 1 and 5 seconds to pause crawling for the rest of the website in respect to other people using it, not to over whelm the website and respectiful behaviour\n",
    "3. There 68 pages on this websitw with 10 people per page, each page crawled data is appended to the CSV file  \n",
    "4. Printed every URL just a check on the code progress make sure its not in a infinity loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=9Os8AKnu__8J&astart=20\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=f0osAKXy__8J&astart=30\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=CVhmAGT0__8J&astart=40\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=GY4kAOb2__8J&astart=50\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=PAQYAHT4__8J&astart=60\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=M-peAKn5__8J&astart=70\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=qtSrAFf6__8J&astart=80\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=Iit5AAb7__8J&astart=90\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=5vUGAHz7__8J&astart=100\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=mV14AOj7__8J&astart=110\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=byRyAET8__8J&astart=120\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=0AiPALH8__8J&astart=130\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=eNkYAOn8__8J&astart=140\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=bWsJAA_9__8J&astart=150\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=SkZ6AGT9__8J&astart=160\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=wmAGAJf9__8J&astart=170\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=yZBIAMv9__8J&astart=180\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=NuAFAPr9__8J&astart=190\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=e2RhADL-__8J&astart=200\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=GpJMAEL-__8J&astart=210\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=iyi4AGT-__8J&astart=220\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=F-13AH_-__8J&astart=230\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=y55KAKD-__8J&astart=240\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=PLJXALP-__8J&astart=250\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=kuVWANT-__8J&astart=260\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=Vi8WAOH-__8J&astart=270\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=mwsIAPf-__8J&astart=280\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=nlTNAAj___8J&astart=290\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=ECtSABv___8J&astart=300\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=DzMoADD___8J&astart=310\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=RTjbAEf___8J&astart=320\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=Y9CEAFb___8J&astart=330\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=6qTLAGX___8J&astart=340\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=wDmEAG3___8J&astart=350\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=ysfPAHf___8J&astart=360\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=uxvqAID___8J&astart=370\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=Us1fAIv___8J&astart=380\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=Li1xAJT___8J&astart=390\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=PZZaAJv___8J&astart=400\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=I6hfAKP___8J&astart=410\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=xpmrAKr___8J&astart=420\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=VuygALL___8J&astart=430\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=v9DuALz___8J&astart=440\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=Yj5sAML___8J&astart=450\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=Y9p7AMj___8J&astart=460\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=8quqAMz___8J&astart=470\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=ZaBrAND___8J&astart=480\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=06nEANX___8J&astart=490\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=dYyAANj___8J&astart=500\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=kkCvANr___8J&astart=510\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=8xCuAN7___8J&astart=520\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=i-M7AOD___8J&astart=530\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=1WjTAOP___8J&astart=540\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=SnR2AOn___8J&astart=550\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=ryXCAOr___8J&astart=560\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=EaPIAOz___8J&astart=570\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=MG18AO7___8J&astart=580\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=Zw2oAPH___8J&astart=590\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=i9vOAPT___8J&astart=600\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=PSV2APb___8J&astart=610\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=vNIMAff___8J&astart=620\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=fzDvAPn___8J&astart=630\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=xzyEAPv___8J&astart=640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=LAh4APz___8J&astart=650\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=5ixoAP3___8J&astart=660\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=H03SAP3___8J&astart=670\n",
      "https://scholar.google.co.uk/citations?view_op=view_org&hl=en&oe=ASCII&org=9117984065169182779&after_author=C2oHAf3___8J&astart=680\n",
      "https://scholar.google.co.ukon\n"
     ]
    }
   ],
   "source": [
    "i= 1\n",
    "while i: \n",
    "    if nextlink != 'None':\n",
    "        stflinks = []\n",
    "        stfname = []\n",
    "        name = []\n",
    "        coven = pd.DataFrame(columns= ['name', 'link','research_field'])\n",
    "        page1 = requests.get(link)\n",
    "        soup1 = BeautifulSoup(page1.text, 'html.parser')\n",
    "        #sub link to a staff page\n",
    "\n",
    "        for main in soup1.find_all('div', class_='gsc_1usr'):\n",
    "            try:\n",
    "                slink = main.h3.a['href']\n",
    "            except Exception as e:\n",
    "                slink = 'None' \n",
    "            stflinks.append(slink)\n",
    "\n",
    "        #staff page extract details(name and research area)   \n",
    "        for stfpage in stflinks:\n",
    "            pagestf= requests.get(\"https://scholar.google.co.uk\" + str(stfpage))\n",
    "            soup2 = BeautifulSoup(pagestf.text, 'html.parser')                      \n",
    "            try:\n",
    "                sname = soup2.find('div', id ='gsc_prf_in').text\n",
    "            except Exception as e:\n",
    "                sname = 'None'  \n",
    "\n",
    "        #all area research in one row         \n",
    "            stfarea = []\n",
    "            for main1 in soup2.find_all(class_='gsc_prf_inta gs_ibl'):\n",
    "                try:\n",
    "                    stfar = main1.text\n",
    "                except Exception as e:\n",
    "                    stfar = 'None'\n",
    "                #stfarea.append('\\n')\n",
    "                stfarea.append(stfar)\n",
    "\n",
    "\n",
    "            links = (\"https://scholar.google.co.uk\" + str(stfpage))\n",
    "            string = \", \"\n",
    "            string = string.join(stfarea)\n",
    "        #Dataframe     \n",
    "            a = {'names': sname, 'link' : links, 'research_field': string}\n",
    "            cov = pd.DataFrame.from_dict(a, orient='index')\n",
    "            coventry = cov.transpose()    \n",
    "            coven = coven.append(coventry, ignore_index= 'True')\n",
    "        coven.to_csv('coventry1.csv', mode='a', header=False)\n",
    "\n",
    "        #Nextpage link\n",
    "        for main3 in soup1.find_all('div', class_= 'gsc_pgn'):\n",
    "            for main3 in soup1.find_all(class_= 'gs_btnPR gs_in_ib gs_btn_half gs_btn_lsb gs_btn_srt gsc_pgn_pnx'):\n",
    "                try:\n",
    "                    nextlink = main3['onclick']            \n",
    "                except Exception as e:\n",
    "                    nextlink = 'None' \n",
    "                start_pos = nextlink.find('=')\n",
    "                temp= nextlink[start_pos + 2:len(nextlink) -1]\n",
    "\n",
    "        def normalize_string(text):\n",
    "\n",
    "            text = text.replace(\"\\\\x3d\", \"=\")    \n",
    "            text = text.replace(\"\\\\x26\", \"&\")    \n",
    "            text = text.strip()\n",
    "\n",
    "            text = text.rstrip('.')\n",
    "            text = unicodedata.normalize(u'NFKD', str(text)).encode('ascii', 'ignore').decode('utf8')\n",
    "\n",
    "            return text \n",
    "        normalize_string(temp)\n",
    "\n",
    "        link = 'https://scholar.google.co.uk' + normalize_string(temp)\n",
    "        print(link)\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "    sleep(randint(1,5))        \n",
    "    i += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Coventry University Faculty data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The faulty page do not have research interest so this has to be crawled differently \n",
    "Have to crawl different faculty personnel separa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faculty of Arts and HumanitiesÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-arts-humanities/persons/?page=1\n"
     ]
    }
   ],
   "source": [
    "faculty= []\n",
    "subfacul= []\n",
    "pnames = []\n",
    "nextpag = []\n",
    "\n",
    "page4 = requests.get(\"https://pureportal.coventry.ac.uk/en/organisations/faculty-of-arts-humanities/persons/\")\n",
    "soup4 = BeautifulSoup(page4.text, 'html.parser')\n",
    "\n",
    "\n",
    "for main4 in soup4.find_all('div', class_='organisation-details'):\n",
    "    try:\n",
    "        facul = main4.h1.text\n",
    "    except Exception as e:\n",
    "        facul = 'None'\n",
    "    faculty.append(facul)    \n",
    "    \n",
    "for main5 in soup4.find_all('h3', class_='title'):\n",
    "    try:\n",
    "        pers = main5.text\n",
    "    except Exception as e:\n",
    "        pers = 'None'\n",
    "    pnames.append(pers)    \n",
    "        \n",
    "for main6 in soup4.find_all('ul',class_='relations organisations'):\n",
    "    try:\n",
    "        perfacul = main6.span.text\n",
    "    except Exception as e:\n",
    "        perfacul = 'None'\n",
    "    subfacul.append(perfacul)\n",
    "\n",
    "#Nextpage link\n",
    "for main7 in soup4.find_all('li', class_='next'):\n",
    "    try:\n",
    "        nextpage = main7.a['href']\n",
    "    except Exception as e:  \n",
    "        nextpage = 'None'\n",
    "    #nextpag.append(nextpage)     \n",
    "    \n",
    "b ={'mainfaculty':faculty, 'pernames':pnames,'subfaculty':subfacul}\n",
    "cov_facul = pd.DataFrame.from_dict(b, orient='index')\n",
    "cov_facult = cov_facul.transpose()\n",
    "cov_facult['faculty'] =cov_facult['mainfaculty'].iloc[0]\n",
    "cov_facult.to_csv('cov_faculty_art.csv')\n",
    "    \n",
    "nextpagelink = (\"https://pureportal.coventry.ac.uk\" + str(nextpage))       \n",
    "print(nextpagelink)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-arts-humanities/persons/?page=2\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-arts-humanities/persons/?page=3\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-arts-humanities/persons/?page=4\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-arts-humanities/persons/?page=5\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-arts-humanities/persons/?page=5\n"
     ]
    }
   ],
   "source": [
    "j = 1\n",
    "while j < 6:\n",
    "    faculty= []\n",
    "    subfacul= []\n",
    "    pnames = []\n",
    "    nextpag = []\n",
    "\n",
    "    page4 = requests.get(nextpagelink)\n",
    "    soup4 = BeautifulSoup(page4.text, 'html.parser')\n",
    "\n",
    "\n",
    "    for main4 in soup4.find_all('div', class_='organisation-details'):\n",
    "        try:\n",
    "            facul = main4.h1.text\n",
    "        except Exception as e:\n",
    "            facul = 'None'\n",
    "        faculty.append(facul)    \n",
    "\n",
    "    for main5 in soup4.find_all('h3', class_='title'):\n",
    "        try:\n",
    "            pers = main5.text\n",
    "        except Exception as e:\n",
    "            pers = 'None'\n",
    "        pnames.append(pers)    \n",
    "\n",
    "    for main6 in soup4.find_all('ul',class_='relations organisations'):\n",
    "        try:\n",
    "            perfacul = main6.span.text\n",
    "        except Exception as e:\n",
    "            perfacul = 'None'\n",
    "        subfacul.append(perfacul)\n",
    "\n",
    "    #Nextpage link\n",
    "    for main7 in soup4.find_all('li', class_='next'):\n",
    "        try:\n",
    "            nextpage = main7.a['href']\n",
    "        except Exception as e:  \n",
    "            nextpage = 'None'\n",
    "        #nextpag.append(nextpage)\n",
    "      \n",
    "\n",
    "    b ={'mainfaculty':faculty, 'pernames':pnames,'subfaculty':subfacul}\n",
    "    cov_facul = pd.DataFrame.from_dict(b, orient='index')\n",
    "    cov_facult = cov_facul.transpose()\n",
    "    cov_facult['faculty'] =cov_facult['mainfaculty'].iloc[0]\n",
    "    cov_facult.to_csv('cov_faculty_art.csv', mode='a', header=False)\n",
    "    \n",
    "    nextpagelink = (\"https://pureportal.coventry.ac.uk\" + str(nextpage))       \n",
    "    print(nextpagelink)\n",
    "    sleep(randint(1,5))\n",
    "    j += 1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mainfaculty</th>\n",
       "      <th>pernames</th>\n",
       "      <th>subfaculty</th>\n",
       "      <th>faculty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Faculty of Arts &amp; Humanities</td>\n",
       "      <td>Jo Sperryn-Jones</td>\n",
       "      <td>School of Art and Design</td>\n",
       "      <td>Faculty of Arts &amp; Humanities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>Esme Spurling</td>\n",
       "      <td>School of Media and Performing Arts</td>\n",
       "      <td>Faculty of Arts &amp; Humanities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>Kathryn Stamp</td>\n",
       "      <td>Faculty Research Centre for Dance Research (CD...</td>\n",
       "      <td>Faculty of Arts &amp; Humanities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>Kaori Stansall</td>\n",
       "      <td>School of Humanities</td>\n",
       "      <td>Faculty of Arts &amp; Humanities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>Jo Gane Steggall</td>\n",
       "      <td>School of Media and Performing Arts</td>\n",
       "      <td>Faculty of Arts &amp; Humanities</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mainfaculty          pernames  \\\n",
       "0  Faculty of Arts & Humanities  Jo Sperryn-Jones   \n",
       "1                          None     Esme Spurling   \n",
       "2                          None     Kathryn Stamp   \n",
       "3                          None    Kaori Stansall   \n",
       "4                          None  Jo Gane Steggall   \n",
       "\n",
       "                                          subfaculty  \\\n",
       "0                           School of Art and Design   \n",
       "1                School of Media and Performing Arts   \n",
       "2  Faculty Research Centre for Dance Research (CD...   \n",
       "3                               School of Humanities   \n",
       "4                School of Media and Performing Arts   \n",
       "\n",
       "                        faculty  \n",
       "0  Faculty of Arts & Humanities  \n",
       "1  Faculty of Arts & Humanities  \n",
       "2  Faculty of Arts & Humanities  \n",
       "3  Faculty of Arts & Humanities  \n",
       "4  Faculty of Arts & Humanities  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_facult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faculty of Business and Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-business-law/persons/?page=1\n"
     ]
    }
   ],
   "source": [
    "faculty= []\n",
    "subfacul= []\n",
    "pnames = []\n",
    "nextpag = []\n",
    "\n",
    "page4 = requests.get(\"https://pureportal.coventry.ac.uk/en/organisations/faculty-of-business-law/persons/\")\n",
    "soup4 = BeautifulSoup(page4.text, 'html.parser')\n",
    "\n",
    "\n",
    "for main4 in soup4.find_all('div', class_='organisation-details'):\n",
    "    try:\n",
    "        facul = main4.h1.text\n",
    "    except Exception as e:\n",
    "        facul = 'None'\n",
    "    faculty.append(facul)    \n",
    "    \n",
    "for main5 in soup4.find_all('h3', class_='title'):\n",
    "    try:\n",
    "        pers = main5.text\n",
    "    except Exception as e:\n",
    "        pers = 'None'\n",
    "    pnames.append(pers)    \n",
    "        \n",
    "for main6 in soup4.find_all('ul',class_='relations organisations'):\n",
    "    try:\n",
    "        perfacul = main6.span.text\n",
    "    except Exception as e:\n",
    "        perfacul = 'None'\n",
    "    subfacul.append(perfacul)\n",
    "\n",
    "#Nextpage link\n",
    "for main7 in soup4.find_all('li', class_='next'):\n",
    "    try:\n",
    "        nextpage = main7.a['href']\n",
    "    except Exception as e:  \n",
    "        nextpage = 'None'\n",
    "    #nextpag.append(nextpage)\n",
    "    \n",
    "    nextpagelink = (\"https://pureportal.coventry.ac.uk\" + str(nextpage))       \n",
    "    print(nextpagelink)\n",
    "    \n",
    "    b ={'mainfaculty':faculty, 'pernames':pnames,'subfaculty':subfacul}\n",
    "    cov_facul = pd.DataFrame.from_dict(b, orient='index')\n",
    "    cov_facult = cov_facul.transpose()\n",
    "    cov_facult['faculty'] =cov_facult['mainfaculty'].iloc[0]\n",
    "    cov_facult.to_csv('cov_faculty_bus.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-business-law/persons/?page=2\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-business-law/persons/?page=3\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-business-law/persons/?page=4\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-business-law/persons/?page=5\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-business-law/persons/?page=6\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-business-law/persons/?page=7\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-business-law/persons/?page=8\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-business-law/persons/?page=9\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-business-law/persons/?page=9\n"
     ]
    }
   ],
   "source": [
    "j = 1\n",
    "while j < 10:\n",
    "    faculty= []\n",
    "    subfacul= []\n",
    "    pnames = []\n",
    "    nextpag = []\n",
    "\n",
    "    page4 = requests.get(nextpagelink)\n",
    "    soup4 = BeautifulSoup(page4.text, 'html.parser')\n",
    "\n",
    "\n",
    "    for main4 in soup4.find_all('div', class_='organisation-details'):\n",
    "        try:\n",
    "            facul = main4.h1.text\n",
    "        except Exception as e:\n",
    "            facul = 'None'\n",
    "        faculty.append(facul)    \n",
    "\n",
    "    for main5 in soup4.find_all('h3', class_='title'):\n",
    "        try:\n",
    "            pers = main5.text\n",
    "        except Exception as e:\n",
    "            pers = 'None'\n",
    "        pnames.append(pers)    \n",
    "\n",
    "    for main6 in soup4.find_all('ul',class_='relations organisations'):\n",
    "        try:\n",
    "            perfacul = main6.span.text\n",
    "        except Exception as e:\n",
    "            perfacul = 'None'\n",
    "        subfacul.append(perfacul)\n",
    "\n",
    "    #Nextpage link\n",
    "    for main7 in soup4.find_all('li', class_='next'):\n",
    "        try:\n",
    "            nextpage = main7.a['href']\n",
    "        except Exception as e:  \n",
    "            nextpage = 'None'\n",
    "        #nextpag.append(nextpage)        \n",
    "\n",
    "    b ={'mainfaculty':faculty, 'pernames':pnames,'subfaculty':subfacul}\n",
    "    cov_facul = pd.DataFrame.from_dict(b, orient='index')\n",
    "    cov_facult = cov_facul.transpose()\n",
    "    cov_facult['faculty'] =cov_facult['mainfaculty'].iloc[0]\n",
    "    cov_facult.to_csv('cov_faculty_bus.csv', mode='a', header=False)\n",
    "\n",
    "\n",
    "    nextpagelink = (\"https://pureportal.coventry.ac.uk\" + str(nextpage))       \n",
    "    print(nextpagelink)\n",
    "    sleep(randint(1,5))\n",
    "    j += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faculty of Engineering, Environment and Computing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-engineering-environment-computing/persons/?page=1\n"
     ]
    }
   ],
   "source": [
    "faculty= []\n",
    "subfacul= []\n",
    "pnames = []\n",
    "nextpag = []\n",
    "\n",
    "page4 = requests.get(\"https://pureportal.coventry.ac.uk/en/organisations/faculty-of-engineering-environment-computing/persons/\")\n",
    "soup4 = BeautifulSoup(page4.text, 'html.parser')\n",
    "\n",
    "\n",
    "for main4 in soup4.find_all('div', class_='organisation-details'):\n",
    "    try:\n",
    "        facul = main4.h1.text\n",
    "    except Exception as e:\n",
    "        facul = 'None'\n",
    "    faculty.append(facul)    \n",
    "    \n",
    "for main5 in soup4.find_all('h3', class_='title'):\n",
    "    try:\n",
    "        pers = main5.text\n",
    "    except Exception as e:\n",
    "        pers = 'None'\n",
    "    pnames.append(pers)    \n",
    "        \n",
    "for main6 in soup4.find_all('ul',class_='relations organisations'):\n",
    "    try:\n",
    "        perfacul = main6.span.text\n",
    "    except Exception as e:\n",
    "        perfacul = 'None'\n",
    "    subfacul.append(perfacul)\n",
    "\n",
    "#Nextpage link\n",
    "for main7 in soup4.find_all('li', class_='next'):\n",
    "    try:\n",
    "        nextpage = main7.a['href']\n",
    "    except Exception as e:  \n",
    "        nextpage = 'None'\n",
    "    #nextpag.append(nextpage)\n",
    "    \n",
    "    nextpagelink = (\"https://pureportal.coventry.ac.uk\" + str(nextpage))       \n",
    "    print(nextpagelink)\n",
    "    \n",
    "    b ={'mainfaculty':faculty, 'pernames':pnames,'subfaculty':subfacul}\n",
    "    cov_facul = pd.DataFrame.from_dict(b, orient='index')\n",
    "    cov_facult = cov_facul.transpose()\n",
    "    cov_facult['faculty'] =cov_facult['mainfaculty'].iloc[0]\n",
    "    cov_facult.to_csv('cov_faculty_eng.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-engineering-environment-computing/persons/?page=2\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-engineering-environment-computing/persons/?page=3\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-engineering-environment-computing/persons/?page=4\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-engineering-environment-computing/persons/?page=5\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-engineering-environment-computing/persons/?page=6\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-engineering-environment-computing/persons/?page=6\n"
     ]
    }
   ],
   "source": [
    "h = 1\n",
    "while h < 7:\n",
    "    faculty= []\n",
    "    subfacul= []\n",
    "    pnames = []\n",
    "    nextpag = []\n",
    "\n",
    "    page4 = requests.get(nextpagelink)\n",
    "    soup4 = BeautifulSoup(page4.text, 'html.parser')\n",
    "\n",
    "\n",
    "    for main4 in soup4.find_all('div', class_='organisation-details'):\n",
    "        try:\n",
    "            facul = main4.h1.text\n",
    "        except Exception as e:\n",
    "            facul = 'None'\n",
    "        faculty.append(facul)    \n",
    "\n",
    "    for main5 in soup4.find_all('h3', class_='title'):\n",
    "        try:\n",
    "            pers = main5.text\n",
    "        except Exception as e:\n",
    "            pers = 'None'\n",
    "        pnames.append(pers)    \n",
    "\n",
    "    for main6 in soup4.find_all('ul',class_='relations organisations'):\n",
    "        try:\n",
    "            perfacul = main6.span.text\n",
    "        except Exception as e:\n",
    "            perfacul = 'None'\n",
    "        subfacul.append(perfacul)\n",
    "\n",
    "    #Nextpage link\n",
    "    for main7 in soup4.find_all('li', class_='next'):\n",
    "        try:\n",
    "            nextpage = main7.a['href']\n",
    "        except Exception as e:  \n",
    "            nextpage = 'None'\n",
    "        #nextpag.append(nextpage)        \n",
    "\n",
    "    b ={'mainfaculty':faculty, 'pernames':pnames,'subfaculty':subfacul}\n",
    "    cov_facul = pd.DataFrame.from_dict(b, orient='index')\n",
    "    cov_facult = cov_facul.transpose()\n",
    "    cov_facult['faculty'] =cov_facult['mainfaculty'].iloc[0]\n",
    "    cov_facult.to_csv('cov_faculty_eng.csv', mode='a', header=False)\n",
    "\n",
    "\n",
    "    nextpagelink = (\"https://pureportal.coventry.ac.uk\" + str(nextpage))       \n",
    "    print(nextpagelink)\n",
    "    sleep(randint(1,5))\n",
    "    h += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faculty of Health and Life Sciences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-health-life-sciences/persons/?page=1\n"
     ]
    }
   ],
   "source": [
    "faculty= []\n",
    "subfacul= []\n",
    "pnames = []\n",
    "nextpag = []\n",
    "\n",
    "page4 = requests.get(\"https://pureportal.coventry.ac.uk/en/organisations/faculty-of-health-life-sciences/persons/\")\n",
    "soup4 = BeautifulSoup(page4.text, 'html.parser')\n",
    "\n",
    "\n",
    "for main4 in soup4.find_all('div', class_='organisation-details'):\n",
    "    try:\n",
    "        facul = main4.h1.text\n",
    "    except Exception as e:\n",
    "        facul = 'None'\n",
    "    faculty.append(facul)    \n",
    "    \n",
    "for main5 in soup4.find_all('h3', class_='title'):\n",
    "    try:\n",
    "        pers = main5.text\n",
    "    except Exception as e:\n",
    "        pers = 'None'\n",
    "    pnames.append(pers)    \n",
    "        \n",
    "for main6 in soup4.find_all('ul',class_='relations organisations'):\n",
    "    try:\n",
    "        perfacul = main6.span.text\n",
    "    except Exception as e:\n",
    "        perfacul = 'None'\n",
    "    subfacul.append(perfacul)\n",
    "\n",
    "#Nextpage link\n",
    "for main7 in soup4.find_all('li', class_='next'):\n",
    "    try:\n",
    "        nextpage = main7.a['href']\n",
    "    except Exception as e:  \n",
    "        nextpage = 'None'\n",
    "    #nextpag.append(nextpage)\n",
    "    \n",
    "    nextpagelink = (\"https://pureportal.coventry.ac.uk\" + str(nextpage))       \n",
    "    print(nextpagelink)\n",
    "    \n",
    "    b ={'mainfaculty':faculty, 'pernames':pnames,'subfaculty':subfacul}\n",
    "    cov_facul = pd.DataFrame.from_dict(b, orient='index')\n",
    "    cov_facult = cov_facul.transpose()\n",
    "    cov_facult['faculty'] =cov_facult['mainfaculty'].iloc[0]\n",
    "    cov_facult.to_csv('cov_faculty_hea.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-health-life-sciences/persons/?page=2\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-health-life-sciences/persons/?page=3\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-health-life-sciences/persons/?page=4\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-health-life-sciences/persons/?page=5\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-health-life-sciences/persons/?page=6\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-health-life-sciences/persons/?page=7\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-health-life-sciences/persons/?page=8\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-health-life-sciences/persons/?page=9\n",
      "https://pureportal.coventry.ac.uk/en/organisations/faculty-of-health-life-sciences/persons/?page=9\n"
     ]
    }
   ],
   "source": [
    "k = 1\n",
    "while k < 10:\n",
    "    faculty= []\n",
    "    subfacul= []\n",
    "    pnames = []\n",
    "    nextpag = []\n",
    "\n",
    "    page4 = requests.get(nextpagelink)\n",
    "    soup4 = BeautifulSoup(page4.text, 'html.parser')\n",
    "\n",
    "\n",
    "    for main4 in soup4.find_all('div', class_='organisation-details'):\n",
    "        try:\n",
    "            facul = main4.h1.text\n",
    "        except Exception as e:\n",
    "            facul = 'None'\n",
    "        faculty.append(facul)    \n",
    "\n",
    "    for main5 in soup4.find_all('h3', class_='title'):\n",
    "        try:\n",
    "            pers = main5.text\n",
    "        except Exception as e:\n",
    "            pers = 'None'\n",
    "        pnames.append(pers)    \n",
    "\n",
    "    for main6 in soup4.find_all('ul',class_='relations organisations'):\n",
    "        try:\n",
    "            perfacul = main6.span.text\n",
    "        except Exception as e:\n",
    "            perfacul = 'None'\n",
    "        subfacul.append(perfacul)\n",
    "\n",
    "    #Nextpage link\n",
    "    for main7 in soup4.find_all('li', class_='next'):\n",
    "        try:\n",
    "            nextpage = main7.a['href']\n",
    "        except Exception as e:  \n",
    "            nextpage = 'None'\n",
    "        #nextpag.append(nextpage)        \n",
    "\n",
    "    b ={'mainfaculty':faculty, 'pernames':pnames,'subfaculty':subfacul}\n",
    "    cov_facul = pd.DataFrame.from_dict(b, orient='index')\n",
    "    cov_facult = cov_facul.transpose()\n",
    "    cov_facult['faculty'] =cov_facult['mainfaculty'].iloc[0]\n",
    "    cov_facult.to_csv('cov_faculty_hea.csv', mode='a', header=False)\n",
    "\n",
    "\n",
    "    nextpagelink = (\"https://pureportal.coventry.ac.uk\" + str(nextpage))       \n",
    "    print(nextpagelink)\n",
    "    sleep(randint(1,5))\n",
    "    k += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coventry research personnel research Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pureportal.coventry.ac.uk/en/persons/?format=&page=1\n"
     ]
    }
   ],
   "source": [
    "stflinks = []\n",
    "names = []\n",
    "\n",
    "page5 = requests.get(\"https://pureportal.coventry.ac.uk/en/persons/\")\n",
    "soup5 = BeautifulSoup(page5.text, 'html.parser')\n",
    "\n",
    "\n",
    "for main5 in soup5.find_all('h3', class_='title'):\n",
    "    try:\n",
    "        slink = main5.a['href']\n",
    "    except Exception as e:\n",
    "        slink = 'None' \n",
    "    stflinks.append(slink)\n",
    "    \n",
    "    try:\n",
    "        stfname=main5.text\n",
    "    except Exception as e:\n",
    "        stfname = 'None' \n",
    "    names.append(stfname)\n",
    "    \n",
    "for main6 in soup5.find_all('li', class_='next'):\n",
    "    try:\n",
    "        nextpg = main6.a['href']\n",
    "    except Exception as e:\n",
    "        nextpg = 'None'\n",
    "            \n",
    "headers = []\n",
    "interest= []\n",
    "required = []\n",
    "for stfpage in stflinks:    \n",
    "    page6 = requests.get(stfpage)\n",
    "    soup6 = BeautifulSoup(page6.text, 'html.parser')\n",
    "    try:\n",
    "        header = soup6.find_all('h3', class_='subheader')[1].text                     \n",
    "    except Exception as e:\n",
    "        header = 'None'     \n",
    "    try:\n",
    "        container = soup6.find_all('div',class_='textblock')[1].text \n",
    "    except Exception as e:\n",
    "        container = 'None'     \n",
    "    requir = header + ': ' + container    \n",
    "    required.append(requir)\n",
    "    \n",
    "          \n",
    "reasearch = pd.DataFrame({\n",
    "    'names_2': names,\n",
    "    'link_CU' : stflinks,\n",
    "    'research_interest': required\n",
    "})\n",
    "reasearch.to_csv('cov_reasearch.csv')\n",
    "\n",
    "links = (\"https://pureportal.coventry.ac.uk\" + str(nextpg))\n",
    "print(links)     \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "while k < 41:\n",
    "    stflinks = []\n",
    "    names = []\n",
    "\n",
    "    page5 = requests.get(links)\n",
    "    soup5 = BeautifulSoup(page5.text, 'html.parser')\n",
    "\n",
    "\n",
    "    for main5 in soup5.find_all('h3', class_='title'):\n",
    "        try:\n",
    "            slink = main5.a['href']\n",
    "        except Exception as e:\n",
    "            slink = 'None' \n",
    "        stflinks.append(slink)\n",
    "\n",
    "        try:\n",
    "            stfname=main5.text\n",
    "        except Exception as e:\n",
    "            stfname = 'None' \n",
    "        names.append(stfname)\n",
    "\n",
    "    for main6 in soup5.find_all('li', class_='next'):\n",
    "        try:\n",
    "            nextpg = main6.a['href']\n",
    "        except Exception as e:\n",
    "            nextpg = 'None'\n",
    "\n",
    "    headers = []\n",
    "    interest= []\n",
    "    required = []\n",
    "    for stfpage in stflinks:    \n",
    "        page6 = requests.get(stfpage)\n",
    "        soup6 = BeautifulSoup(page6.text, 'html.parser')\n",
    "        try:\n",
    "            header = soup6.find_all('h3', class_='subheader')[1].text                     \n",
    "        except Exception as e:\n",
    "            header = 'None'     \n",
    "        try:\n",
    "            container = soup6.find_all('div',class_='textblock')[1].text \n",
    "        except Exception as e:\n",
    "            container = 'None'     \n",
    "        requir = header + ': ' + container    \n",
    "        required.append(requir)\n",
    "\n",
    "\n",
    "    reasearch = pd.DataFrame({\n",
    "        'names_2': names,\n",
    "        'link_CU' : stflinks,\n",
    "        'research_interest': required\n",
    "    })\n",
    "    reasearch.to_csv('cov_reasearch.csv', mode='a', header=False)\n",
    "\n",
    "    links = (\"https://pureportal.coventry.ac.uk\" + str(nextpg))\n",
    "    #print(links)\n",
    "    sleep(randint(1,5))\n",
    "    k += 1   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the Faculty csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "Finded_URL = ['cov_faculty_art.csv','cov_faculty_bus.csv','cov_faculty_eng.csv','cov_faculty_hea.csv' ]\n",
    "\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f,header=0) for f in Finded_URL])\n",
    "combined_csv.head()\n",
    "combined_csv.to_csv( \"Finded_URL_All.csv\", quotechar='\"',\n",
    "                    quoting=csv.QUOTE_ALL, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
