{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEARCH ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Will combine all the 3 CSV based on the names from the google scholar crawling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the ID column and drop the page index from crawling \n",
    "coventry_all = pd.read_csv('coventry1.csv')\n",
    "coventry_all['ID'] = [x for x in range(1, len(coventry_all.values)+1)]\n",
    "coventry_all.drop(coventry_all.columns[0], axis=1, inplace=True)\n",
    "coventry_all.research_field=coventry_all.research_field.astype(str) #avoid been seen as a float \n",
    "coventry_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Google Scholar data with Faculty data from CU based on researchers present in Google scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1a = pd.DataFrame(pd.read_csv('coventry1.csv'))\n",
    "f2a = pd.DataFrame(pd.read_csv('Finded_URL_All.csv'))\n",
    "facultDept = pd.merge(f1a, f2a, how='left', left_on='names', right_on='pernames')\n",
    "drop_cols = ['Unnamed: 0_x', 'Unnamed: 0_y', 'mainfaculty']\n",
    "facultyDept = facultDept.drop(drop_cols, axis=1)\n",
    "facultyDept = facultyDept.drop_duplicates(subset = 'names', keep = 'first') #remove duplicates\n",
    "facultyDept.to_csv('facultyDept.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facultyDept.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3a = pd.DataFrame(pd.read_csv('facultyDept.csv'))\n",
    "f4a = pd.DataFrame(pd.read_csv('cov_reasearch.csv'))\n",
    "facultDept_1 = pd.merge(f3a, f4a, how='left', left_on='names', right_on='names_2')\n",
    "drop_cols = ['Unnamed: 0_x', 'Unnamed: 0_y','pernames','names_2']\n",
    "facultyDept_1 = facultDept_1.drop(drop_cols, axis=1)\n",
    "facultyDept_1.to_csv('cov_facultyDept.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facultyDept_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>link</th>\n",
       "      <th>research_field</th>\n",
       "      <th>subfaculty</th>\n",
       "      <th>faculty</th>\n",
       "      <th>link_CU</th>\n",
       "      <th>research_interest</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Timothy Mason</td>\n",
       "      <td>https://scholar.google.co.uk/citations?hl=en&amp;u...</td>\n",
       "      <td>sonochemistry, ultrasound, chemistry, environm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gurnam Singh</td>\n",
       "      <td>https://scholar.google.co.uk/citations?hl=en&amp;u...</td>\n",
       "      <td>social work, race and racism, critical pedagog...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://pureportal.coventry.ac.uk/en/persons/g...</td>\n",
       "      <td>Research Interests: Emancipatory research and ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WD Li</td>\n",
       "      <td>https://scholar.google.co.uk/citations?hl=en&amp;u...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dr. Mohammad M Ali</td>\n",
       "      <td>https://scholar.google.co.uk/citations?hl=en&amp;u...</td>\n",
       "      <td>Forecast Information Sharing, ARIMA Modelling,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petra Wark</td>\n",
       "      <td>https://scholar.google.co.uk/citations?hl=en&amp;u...</td>\n",
       "      <td>m/eHealth, epidemiology, primary prevention, d...</td>\n",
       "      <td>Faculty Research Centre for Intelligent Health...</td>\n",
       "      <td>Faculty of Health &amp; Life Sciences</td>\n",
       "      <td>https://pureportal.coventry.ac.uk/en/persons/p...</td>\n",
       "      <td>Research Interests: Efficacy and effectiveness...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                names                                               link  \\\n",
       "0       Timothy Mason  https://scholar.google.co.uk/citations?hl=en&u...   \n",
       "1        Gurnam Singh  https://scholar.google.co.uk/citations?hl=en&u...   \n",
       "2               WD Li  https://scholar.google.co.uk/citations?hl=en&u...   \n",
       "3  Dr. Mohammad M Ali  https://scholar.google.co.uk/citations?hl=en&u...   \n",
       "4          Petra Wark  https://scholar.google.co.uk/citations?hl=en&u...   \n",
       "\n",
       "                                      research_field  \\\n",
       "0  sonochemistry, ultrasound, chemistry, environm...   \n",
       "1  social work, race and racism, critical pedagog...   \n",
       "2                                                nan   \n",
       "3  Forecast Information Sharing, ARIMA Modelling,...   \n",
       "4  m/eHealth, epidemiology, primary prevention, d...   \n",
       "\n",
       "                                          subfaculty  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  Faculty Research Centre for Intelligent Health...   \n",
       "\n",
       "                             faculty  \\\n",
       "0                                NaN   \n",
       "1                                NaN   \n",
       "2                                NaN   \n",
       "3                                NaN   \n",
       "4  Faculty of Health & Life Sciences   \n",
       "\n",
       "                                             link_CU  \\\n",
       "0                                                NaN   \n",
       "1  https://pureportal.coventry.ac.uk/en/persons/g...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  https://pureportal.coventry.ac.uk/en/persons/p...   \n",
       "\n",
       "                                   research_interest  ID  \n",
       "0                                                NaN   1  \n",
       "1  Research Interests: Emancipatory research and ...   2  \n",
       "2                                                NaN   3  \n",
       "3                                                NaN   4  \n",
       "4  Research Interests: Efficacy and effectiveness...   5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add the ID column and drop the page index from crawling \n",
    "coventry_all_2 = pd.read_csv('cov_facultyDept.csv')\n",
    "coventry_all_2['ID'] = [x for x in range(1, len(coventry_all_2.values)+1)]\n",
    "coventry_all_2.drop(coventry_all_2.columns[0], axis=1, inplace=True)\n",
    "coventry_all_2.research_field=coventry_all_2.research_field.astype(str) #avoid been seen as a float \n",
    "coventry_all_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the google scholar data (with faculty and research interest if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking missing data. Missing data are not going to be dropped as student might be looking for name of the professor \n",
    "print(coventry_all_2.dtypes)\n",
    "print('-------------------------')\n",
    "# to see where you're missing data and how much data is missing \n",
    "print(coventry_all_2.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coventry_all.rename(columns={'name':'names'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INDEXER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An indexer of a search engine it's basically a smart storage of our data which we can later easily retrieve data given a search query. It parses the name and research field of the data scraped by the crawler to single words. All these words make up the vocabulary of our index. \n",
    "Next step is to put the ID of the data in the posting lists of the words that the data contains. For example data called \"This happened today\" will be stored in posting lists of terms \"this\", \"happened\" and \"today\". \n",
    "Before creating the index we preprocess the text of the data in order to get rid of useless information. We the text of accents and turn everything to lowercase. Next we perform lemmatization. This is slightly smarter version of stemming. Essentially, it's a word normalization, e.g. all nouns to singular, all verbs in present tense etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "coventry_processed = coventry_all_2.copy()\n",
    "coventry_all_2a = coventry_all_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_entry = coventry_processed.loc[0,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "names                                                    Timothy Mason\n",
       "link                 https://scholar.google.co.uk/citations?hl=en&u...\n",
       "research_field       sonochemistry, ultrasound, chemistry, environm...\n",
       "subfaculty                                                         NaN\n",
       "faculty                                                            NaN\n",
       "link_CU                                                            NaN\n",
       "research_interest                                                  NaN\n",
       "ID                                                                   1\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn text to lowercase and remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(text):\n",
    "    text = text.lower() #to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) #strip punctuation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sonochemistry ultrasound chemistry environment food technology'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_string(single_entry.research_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lemmatize, i.e. word normalization.\n",
    "\n",
    "This method requires some additional information about the words. We need to find the word category of each word, e.g. verb, noun etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple: n\n",
      " Run: v\n",
      " Happy: a\n"
     ]
    }
   ],
   "source": [
    "print(\"Apple: {}\\n Run: {}\\n Happy: {}\" .format(get_wordnet_pos(\"apple\"), get_wordnet_pos(\"run\"), get_wordnet_pos(\"happy\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to remove stopwords, i.e. words with low informational value and repeats alot making the document bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll iterate over all words in text, lemmatize and return the transformed string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def stop_lemmatize(doc):\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    tmp = \"\"\n",
    "    for w in tokens:\n",
    "        if w not in stop:\n",
    "            tmp += lem.lemmatize(w, get_wordnet_pos(w)) + \" \"\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sonochemistry , ultrasound , chemistry , environment , food technology '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_lemmatize(doc = single_entry.research_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sonochemistry ultrasound chemistry environment food technology'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time process_string(single_entry.research_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the process_string function to all names and research fields in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coventry_all_2.dtypes)\n",
    "print('--------------------------')\n",
    "print(coventry_processed.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(df):\n",
    "    df['names'] = df['names'].apply(process_string)\n",
    "    df['research_field'] = df['research_field'].apply(process_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.98 ms\n"
     ]
    }
   ],
   "source": [
    "%time transform_df(coventry_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coventry_processed.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Now we can iterate over all entries to create the index. We'll go step by step again before wrapping it all in one nice function.\n",
    "Merge title and summary into one field and drop all columns except for ID as we don't need those anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "coventry_processed['text'] = coventry_processed['names'] + \" \" + coventry_processed['research_field']\n",
    "drop_cols = ['names','link', 'research_field', 'subfaculty', 'faculty', 'link_CU', 'research_interest']\n",
    "coventry_processed = coventry_processed.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coventry_processed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(df):\n",
    "    df = df\n",
    "    df['names'] = df['names'].apply(process_string)\n",
    "    df['research_field'] = df['research_field'].apply(process_string)\n",
    "    df['text'] = df['names'] + \" \" + df['research_field']\n",
    "    drop_cols = ['names', 'research_field', 'link']\n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll build index with just one entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                                      1\n",
      "text    timothy mason sonochemistry ultrasound chemist...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "single_entry = coventry_processed.loc[0,:].copy()\n",
    "print(single_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the entry to single words and return list and save entry's ID as object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['timothy', 'mason', 'sonochemistry', 'ultrasound', 'chemistry', 'environment', 'food', 'technology']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "words = single_entry.text.split()\n",
    "ID = single_entry.ID\n",
    "print(words)\n",
    "print(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word in index' vocabulary is a dictionary key and has its own posting list with IDs. Let's construct one word vocabulary as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timothy': [1]}\n"
     ]
    }
   ],
   "source": [
    "word = words[0]\n",
    "sample = {word: [ID]}\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate over all words and if they aren't in the vocabulary yet we add them. Also for each word we append the entry ID to the posting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test = {}\n",
    "for word in words:\n",
    "    if word in index_test.keys():\n",
    "        index_test[word].append(ID)\n",
    "    else:\n",
    "        index_test[word] = [ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timothy': [1], 'mason': [1], 'sonochemistry': [1], 'ultrasound': [1], 'chemistry': [1], 'environment': [1], 'food': [1], 'technology': [1]}\n"
     ]
    }
   ],
   "source": [
    "print(index_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now this process can be repeated for all entries in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_itd(single_entry, index):\n",
    "    words = single_entry.text.split()\n",
    "    ID = single_entry.ID\n",
    "    for word in words:\n",
    "        if word in index.keys():\n",
    "            index[word].append(ID)\n",
    "        else:\n",
    "            index[word] = [ID]\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'machine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-6af30a69fa5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mindex_itd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmachine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'machine' is not defined"
     ]
    }
   ],
   "source": [
    "index_itd(machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timothy': [1], 'mason': [1], 'sonochemistry': [1], 'ultrasound': [1], 'chemistry': [1], 'environment': [1], 'food': [1], 'technology': [1]}\n"
     ]
    }
   ],
   "source": [
    "ind = index_itd(single_entry=single_entry, index= {})\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can iterate over all entries in the database, process them append to index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_all(df, index):\n",
    "    for i in range(len(df)):\n",
    "        single_entry = df.loc[i,:]\n",
    "        index = index_itd(single_entry = single_entry, index = index)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2761"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = index_all(coventry_processed, index = {})\n",
    "len(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we wrap everything in one nice function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(df, index):\n",
    "    to_add = transform_df(df)\n",
    "    index = index_all(df = to_add, index = index)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = build_index(df = coventry_all_2, index = {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2761"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ranked retrieval\n",
    "The user would probably prefer the more relevant pages to be displayed before those that are less relevant (hopefully they're at least a bit relevant). For our search engine to support such option we need to store some information about the scraped documents that could be later used for this purpose. We'll use averaged word2vec for this purpose. Word2Vec model is single hidden-layer neural network. The hidden layer is actually what is so useful about this model. Given a word the layer's activation gives a unique vector that word. For each document we can iterate over all words, extract their vectors and then by averaging obtain a document vector. Compared to other methods averaged word2vec has multiple advantages. Unlike simpler methods such as bag-of-words, n-grams and tf-idf the size of the vectors is fixed. For example bag-of-words is also using vectors but the size of these vectors equals the number of unique words in the corpus. This means that the computational and storage requirements get larger as the corpus gets larger. Averaged word2vec is also able to represent the documents on more abstract level than simpler methods and should therefore provide better method of ranking. We're using word2vec rather than doc2vec because we can simply use pretrained word2vec model to compute the document vectors. Using doc2vec would mean training a neural network from scratch which requires computational power, time and rather large dataset.\n",
    "\n",
    "Import and download pretrained word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOWNLOAD PRETRAINED  NETWORK FROM https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit= 10**5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Try getting vectors for all words in the text and averaging to get single vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_vectors(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.vocab]\n",
    "    if len(doc) == 0:\n",
    "        return np.zeros(300)\n",
    "    else:\n",
    "        return np.mean(word2vec_model[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time test_vec = average_vectors(word2vec, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate over documents, compute their vectors and construct a document vectors database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ranking(df):\n",
    "    corpus = df[['ID', 'text']].copy()\n",
    "    doc_vecs = {}\n",
    "    for i in range(len(corpus)):\n",
    "        row = corpus.loc[i,:]\n",
    "        text = row.text.split()\n",
    "        doc_vecs[row.ID]=average_vectors(word2vec, text)\n",
    "    doc_vecs = pd.DataFrame.from_dict(data=doc_vecs, orient=\"index\")\n",
    "    doc_vecs['ID'] = doc_vecs.index\n",
    "    return doc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.004995</td>\n",
       "      <td>0.145549</td>\n",
       "      <td>-0.034017</td>\n",
       "      <td>0.059591</td>\n",
       "      <td>-0.135661</td>\n",
       "      <td>0.157700</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>-0.249349</td>\n",
       "      <td>-0.066030</td>\n",
       "      <td>-0.110738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062927</td>\n",
       "      <td>-0.138468</td>\n",
       "      <td>0.027649</td>\n",
       "      <td>0.030111</td>\n",
       "      <td>0.122528</td>\n",
       "      <td>0.026530</td>\n",
       "      <td>-0.037150</td>\n",
       "      <td>0.069295</td>\n",
       "      <td>0.086675</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022522</td>\n",
       "      <td>-0.079918</td>\n",
       "      <td>0.035995</td>\n",
       "      <td>0.025421</td>\n",
       "      <td>0.027618</td>\n",
       "      <td>0.024967</td>\n",
       "      <td>0.107208</td>\n",
       "      <td>-0.074802</td>\n",
       "      <td>0.044128</td>\n",
       "      <td>-0.098450</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015661</td>\n",
       "      <td>-0.152939</td>\n",
       "      <td>0.086594</td>\n",
       "      <td>-0.089752</td>\n",
       "      <td>0.048096</td>\n",
       "      <td>0.095215</td>\n",
       "      <td>-0.024162</td>\n",
       "      <td>0.064499</td>\n",
       "      <td>0.040466</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.037937</td>\n",
       "      <td>-0.024832</td>\n",
       "      <td>-0.023218</td>\n",
       "      <td>0.063184</td>\n",
       "      <td>-0.047705</td>\n",
       "      <td>0.057776</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>-0.115820</td>\n",
       "      <td>0.038379</td>\n",
       "      <td>0.114746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115570</td>\n",
       "      <td>-0.063135</td>\n",
       "      <td>0.002841</td>\n",
       "      <td>0.117480</td>\n",
       "      <td>-0.059705</td>\n",
       "      <td>0.042377</td>\n",
       "      <td>-0.107935</td>\n",
       "      <td>0.098145</td>\n",
       "      <td>-0.098505</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.181356</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>-0.045186</td>\n",
       "      <td>0.107686</td>\n",
       "      <td>0.062350</td>\n",
       "      <td>0.107951</td>\n",
       "      <td>0.099264</td>\n",
       "      <td>-0.093320</td>\n",
       "      <td>0.035726</td>\n",
       "      <td>-0.056599</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031942</td>\n",
       "      <td>0.058922</td>\n",
       "      <td>0.080709</td>\n",
       "      <td>0.116130</td>\n",
       "      <td>-0.001582</td>\n",
       "      <td>-0.014516</td>\n",
       "      <td>-0.065694</td>\n",
       "      <td>0.179199</td>\n",
       "      <td>0.082611</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>0.062280</td>\n",
       "      <td>0.011563</td>\n",
       "      <td>0.048730</td>\n",
       "      <td>0.213477</td>\n",
       "      <td>-0.025598</td>\n",
       "      <td>0.067212</td>\n",
       "      <td>0.133105</td>\n",
       "      <td>-0.093530</td>\n",
       "      <td>0.051514</td>\n",
       "      <td>0.089893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031836</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>-0.027441</td>\n",
       "      <td>0.049124</td>\n",
       "      <td>-0.033838</td>\n",
       "      <td>0.100439</td>\n",
       "      <td>-0.083398</td>\n",
       "      <td>0.181696</td>\n",
       "      <td>0.035620</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>0.133586</td>\n",
       "      <td>0.060221</td>\n",
       "      <td>0.068899</td>\n",
       "      <td>0.165365</td>\n",
       "      <td>0.050496</td>\n",
       "      <td>-0.005371</td>\n",
       "      <td>0.177653</td>\n",
       "      <td>-0.137044</td>\n",
       "      <td>0.068766</td>\n",
       "      <td>-0.005046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110748</td>\n",
       "      <td>-0.045492</td>\n",
       "      <td>0.017721</td>\n",
       "      <td>0.051107</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>0.089681</td>\n",
       "      <td>-0.021891</td>\n",
       "      <td>-0.004557</td>\n",
       "      <td>-0.285156</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>-0.007812</td>\n",
       "      <td>0.117554</td>\n",
       "      <td>-0.021118</td>\n",
       "      <td>0.103394</td>\n",
       "      <td>0.010925</td>\n",
       "      <td>0.085999</td>\n",
       "      <td>0.187927</td>\n",
       "      <td>-0.206055</td>\n",
       "      <td>0.109253</td>\n",
       "      <td>-0.177490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095642</td>\n",
       "      <td>-0.137299</td>\n",
       "      <td>0.109314</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>-0.052979</td>\n",
       "      <td>0.167908</td>\n",
       "      <td>-0.046844</td>\n",
       "      <td>0.260986</td>\n",
       "      <td>0.230835</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>-0.028483</td>\n",
       "      <td>0.090603</td>\n",
       "      <td>0.035319</td>\n",
       "      <td>0.136081</td>\n",
       "      <td>-0.021756</td>\n",
       "      <td>0.030965</td>\n",
       "      <td>0.196520</td>\n",
       "      <td>-0.060208</td>\n",
       "      <td>0.053752</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046875</td>\n",
       "      <td>-0.014472</td>\n",
       "      <td>-0.046319</td>\n",
       "      <td>0.026260</td>\n",
       "      <td>0.168349</td>\n",
       "      <td>0.006951</td>\n",
       "      <td>-0.041292</td>\n",
       "      <td>0.089952</td>\n",
       "      <td>0.041477</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>-0.014557</td>\n",
       "      <td>-0.044067</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.148926</td>\n",
       "      <td>-0.175049</td>\n",
       "      <td>0.212860</td>\n",
       "      <td>0.190063</td>\n",
       "      <td>-0.159180</td>\n",
       "      <td>0.226074</td>\n",
       "      <td>0.206543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203369</td>\n",
       "      <td>-0.098999</td>\n",
       "      <td>-0.048889</td>\n",
       "      <td>-0.131378</td>\n",
       "      <td>-0.070068</td>\n",
       "      <td>-0.182983</td>\n",
       "      <td>-0.104126</td>\n",
       "      <td>-0.119141</td>\n",
       "      <td>0.053772</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>682 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "1   -0.004995  0.145549 -0.034017  0.059591 -0.135661  0.157700  0.003092   \n",
       "2    0.022522 -0.079918  0.035995  0.025421  0.027618  0.024967  0.107208   \n",
       "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4   -0.037937 -0.024832 -0.023218  0.063184 -0.047705  0.057776  0.000638   \n",
       "5   -0.181356  0.044922 -0.045186  0.107686  0.062350  0.107951  0.099264   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "678  0.062280  0.011563  0.048730  0.213477 -0.025598  0.067212  0.133105   \n",
       "679  0.133586  0.060221  0.068899  0.165365  0.050496 -0.005371  0.177653   \n",
       "680 -0.007812  0.117554 -0.021118  0.103394  0.010925  0.085999  0.187927   \n",
       "681 -0.028483  0.090603  0.035319  0.136081 -0.021756  0.030965  0.196520   \n",
       "682 -0.014557 -0.044067  0.070312  0.148926 -0.175049  0.212860  0.190063   \n",
       "\n",
       "            7         8         9  ...       291       292       293  \\\n",
       "1   -0.249349 -0.066030 -0.110738  ...  0.062927 -0.138468  0.027649   \n",
       "2   -0.074802  0.044128 -0.098450  ... -0.015661 -0.152939  0.086594   \n",
       "3    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "4   -0.115820  0.038379  0.114746  ...  0.115570 -0.063135  0.002841   \n",
       "5   -0.093320  0.035726 -0.056599  ... -0.031942  0.058922  0.080709   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "678 -0.093530  0.051514  0.089893  ...  0.031836  0.012695 -0.027441   \n",
       "679 -0.137044  0.068766 -0.005046  ... -0.110748 -0.045492  0.017721   \n",
       "680 -0.206055  0.109253 -0.177490  ...  0.095642 -0.137299  0.109314   \n",
       "681 -0.060208  0.053752  0.002587  ... -0.046875 -0.014472 -0.046319   \n",
       "682 -0.159180  0.226074  0.206543  ...  0.203369 -0.098999 -0.048889   \n",
       "\n",
       "          294       295       296       297       298       299   ID  \n",
       "1    0.030111  0.122528  0.026530 -0.037150  0.069295  0.086675    1  \n",
       "2   -0.089752  0.048096  0.095215 -0.024162  0.064499  0.040466    2  \n",
       "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    3  \n",
       "4    0.117480 -0.059705  0.042377 -0.107935  0.098145 -0.098505    4  \n",
       "5    0.116130 -0.001582 -0.014516 -0.065694  0.179199  0.082611    5  \n",
       "..        ...       ...       ...       ...       ...       ...  ...  \n",
       "678  0.049124 -0.033838  0.100439 -0.083398  0.181696  0.035620  678  \n",
       "679  0.051107  0.009928  0.089681 -0.021891 -0.004557 -0.285156  679  \n",
       "680  0.003357 -0.052979  0.167908 -0.046844  0.260986  0.230835  680  \n",
       "681  0.026260  0.168349  0.006951 -0.041292  0.089952  0.041477  681  \n",
       "682 -0.131378 -0.070068 -0.182983 -0.104126 -0.119141  0.053772  682  \n",
       "\n",
       "[682 rows x 301 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vecs = prepare_ranking(df=coventry_all_2)\n",
    "doc_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Query processor\n",
    "The final part of a search engine is a query processor which actually performs the search task. Given a query by user the processor should return list of relevant documents. There are multiple types of queries. We'll start with a simple \"google-ish\" query where assume the user looks for documents relevant to all words in the query. Therefore we transform the query to boolean by connecting all words with AND operator.\n",
    "\n",
    "First, the processor preprocesses the query the same way as the indexer preprocessed the text. In other words, we normalize the query to match the format of text in the index. Next, the query is parsed to single words. We look into index if these words are part of the vocabulary. If a word is in index we retrieve its posting list. Finally, we look for intersection of all retrieved posting lists. The result is list of document IDs that the user asked for. However, we need to return something more useful than just a list of IDs. Therefore,we retrieve the information stored about the documents in the university database. Before printing the results we should also rank the documents. This ranking should be based on relevance to query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement:\n",
    "    Boolean query\n",
    "    phrase matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Normalize query¶\n",
    "define an example query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Christopher Clarke\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: Christopher Clarke.\n",
      "Normalized query: christopher clarke.\n"
     ]
    }
   ],
   "source": [
    "print(\"User query: {}.\" .format(test))\n",
    "test_norm = process_string(test)\n",
    "print(\"Normalized query: {}.\" .format(test_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the query into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = test_norm.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we wrap this in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    norm = process_string(query)\n",
    "    return norm.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retrieve from index¶\n",
    "And we iterate over the words, looking if they're in the index vocabulary. If so then we retrieve the associated posting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24, 37, 504]]\n",
      "[[24, 37, 504], [93, 213]]\n"
     ]
    }
   ],
   "source": [
    "retrieved = []\n",
    "for word in test_split:\n",
    "    if word in index.keys():\n",
    "        retrieved.append(index[word])\n",
    "        print(retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look for the intersection of all posting lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lists_intersection(lists):\n",
    "    intersect = list(set.intersection(*map(set, lists)))\n",
    "    intersect.sort()\n",
    "    return intersect\n",
    "lists_intersection(retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap this part in a function before proceeding to formatting the results. The additional if statement is for cases when there's nothing retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_googleish(query, index=idx):\n",
    "    query_split = process_query(query)\n",
    "    retrieved = []\n",
    "    for word in query_split:\n",
    "        if word in index.keys():\n",
    "            retrieved.append(index[word])\n",
    "    if len(retrieved)>0:\n",
    "        result = lists_intersection(retrieved)\n",
    "    else:\n",
    "        result = ['No Information Found']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No Information Found']\n"
     ]
    }
   ],
   "source": [
    "result_IDs = search_googleish(\"virus\", index)\n",
    "print(result_IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: If there's no document retrieved, try removing one term and looking for simplified query + tell user that such document doesn't include term X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve from our database\n",
    "Now we need to connect the retrieved IDs with some useful information stored in database that we first use to refine the results and then to print nice result to user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>link</th>\n",
       "      <th>research_field</th>\n",
       "      <th>subfaculty</th>\n",
       "      <th>faculty</th>\n",
       "      <th>link_CU</th>\n",
       "      <th>research_interest</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Timothy Mason</td>\n",
       "      <td>https://scholar.google.co.uk/citations?hl=en&amp;u...</td>\n",
       "      <td>sonochemistry, ultrasound, chemistry, environm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gurnam Singh</td>\n",
       "      <td>https://scholar.google.co.uk/citations?hl=en&amp;u...</td>\n",
       "      <td>social work, race and racism, critical pedagog...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://pureportal.coventry.ac.uk/en/persons/g...</td>\n",
       "      <td>Research Interests: Emancipatory research and ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WD Li</td>\n",
       "      <td>https://scholar.google.co.uk/citations?hl=en&amp;u...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dr. Mohammad M Ali</td>\n",
       "      <td>https://scholar.google.co.uk/citations?hl=en&amp;u...</td>\n",
       "      <td>Forecast Information Sharing, ARIMA Modelling,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petra Wark</td>\n",
       "      <td>https://scholar.google.co.uk/citations?hl=en&amp;u...</td>\n",
       "      <td>m/eHealth, epidemiology, primary prevention, d...</td>\n",
       "      <td>Faculty Research Centre for Intelligent Health...</td>\n",
       "      <td>Faculty of Health &amp; Life Sciences</td>\n",
       "      <td>https://pureportal.coventry.ac.uk/en/persons/p...</td>\n",
       "      <td>Research Interests: Efficacy and effectiveness...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                names                                               link  \\\n",
       "0       Timothy Mason  https://scholar.google.co.uk/citations?hl=en&u...   \n",
       "1        Gurnam Singh  https://scholar.google.co.uk/citations?hl=en&u...   \n",
       "2               WD Li  https://scholar.google.co.uk/citations?hl=en&u...   \n",
       "3  Dr. Mohammad M Ali  https://scholar.google.co.uk/citations?hl=en&u...   \n",
       "4          Petra Wark  https://scholar.google.co.uk/citations?hl=en&u...   \n",
       "\n",
       "                                      research_field  \\\n",
       "0  sonochemistry, ultrasound, chemistry, environm...   \n",
       "1  social work, race and racism, critical pedagog...   \n",
       "2                                                nan   \n",
       "3  Forecast Information Sharing, ARIMA Modelling,...   \n",
       "4  m/eHealth, epidemiology, primary prevention, d...   \n",
       "\n",
       "                                          subfaculty  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  Faculty Research Centre for Intelligent Health...   \n",
       "\n",
       "                             faculty  \\\n",
       "0                                NaN   \n",
       "1                                NaN   \n",
       "2                                NaN   \n",
       "3                                NaN   \n",
       "4  Faculty of Health & Life Sciences   \n",
       "\n",
       "                                             link_CU  \\\n",
       "0                                                NaN   \n",
       "1  https://pureportal.coventry.ac.uk/en/persons/g...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  https://pureportal.coventry.ac.uk/en/persons/p...   \n",
       "\n",
       "                                   research_interest  ID  \n",
       "0                                                NaN   1  \n",
       "1  Research Interests: Emancipatory research and ...   2  \n",
       "2                                                NaN   3  \n",
       "3                                                NaN   4  \n",
       "4  Research Interests: Efficacy and effectiveness...   5  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is our database\n",
    "#meta = coventry_all_2.drop(['text'], axis=1).copy()\n",
    "meta = coventry_all_2a.copy()\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query from database to get only rows of retrieved IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_id_df(retrieved_id, df):\n",
    "    return df[df.ID.isin(retrieved_id)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>link</th>\n",
       "      <th>research_field</th>\n",
       "      <th>subfaculty</th>\n",
       "      <th>faculty</th>\n",
       "      <th>link_CU</th>\n",
       "      <th>research_interest</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [names, link, research_field, subfaculty, faculty, link_CU, research_interest, ID]\n",
       "Index: []"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_meta = connect_id_df(result_IDs, meta)\n",
    "result_meta.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranked retrieval\n",
    "Now we return back to the word2vec vectors we computed after indexing the documents. We'l compute the vector for the query as well and then using a cosine similarity compare query to retrieved document relevance.\n",
    "\n",
    "Compute vector for query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vec = average_vectors(word2vec, test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve vectors of retrieve documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_vecs = connect_id_df(result_IDs, doc_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute cosine similarity between retrieved documents and query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(a, b):\n",
    "    dot = np.dot(a, b)\n",
    "    norma = np.linalg.norm(a)\n",
    "    normb = np.linalg.norm(b)\n",
    "    cos = dot / (norma * normb)\n",
    "    return(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = []\n",
    "for i in range(len(result_vecs)):\n",
    "    doc_vec = result_vecs.loc[i,:].drop(['ID'])\n",
    "    cos_sim.append(cos_similarity(doc_vec, query_vec))\n",
    "result_meta['rank'] = cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort retrieved docs by cosine similarity which is proxi for relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>link</th>\n",
       "      <th>research_field</th>\n",
       "      <th>subfaculty</th>\n",
       "      <th>faculty</th>\n",
       "      <th>link_CU</th>\n",
       "      <th>research_interest</th>\n",
       "      <th>ID</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [names, link, research_field, subfaculty, faculty, link_CU, research_interest, ID, rank]\n",
       "Index: []"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_meta.sort_values('rank', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>link</th>\n",
       "      <th>research_field</th>\n",
       "      <th>subfaculty</th>\n",
       "      <th>faculty</th>\n",
       "      <th>link_CU</th>\n",
       "      <th>research_interest</th>\n",
       "      <th>ID</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [names, link, research_field, subfaculty, faculty, link_CU, research_interest, ID, rank]\n",
       "Index: []"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_dept_1 = pd.merge(coventry_all_2a, result_meta, how='right', left_on='ID', right_on='ID')\n",
    "cov_dept_1 = cov_dept_1[cov_dept_1.columns.drop(list(cov_dept_1.filter(regex='_y')))]\n",
    "cov_dept_1.columns = cov_dept_1.columns.str.replace('_x','')\n",
    "#cov_dept_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap this in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_results(query, results):\n",
    "    query_norm = process_query(query)\n",
    "    query_vec = average_vectors(word2vec, query_norm)\n",
    "    result_vecs = connect_id_df(results.ID, doc_vecs)\n",
    "    cos_sim = []\n",
    "    for i in range(len(result_vecs)):\n",
    "        doc_vec = result_vecs.loc[i,:].drop(['ID'])\n",
    "        cos_sim.append(cos_similarity(doc_vec, query_vec))\n",
    "    results['rank'] = cos_sim\n",
    "    results = results.sort_values('rank', axis=0)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = rank_results(\"Christopher Clarke\", result_meta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print results to user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(result_df):\n",
    "    for i in range(len(result_df)):\n",
    "        res = result_df.loc[i, :]\n",
    "        print(res.names)\n",
    "        print(\"research field: \", res.research_field)\n",
    "        print('%.200s'  % res.research_interest )\n",
    "        print(\"Subfaculty: \", res.subfaculty)\n",
    "        print(\"Faculty: \", res.subfaculty)\n",
    "        print(\"CU link: \", res.link_CU)\n",
    "        print(\"GScolar link\", res.link)\n",
    "        #if i == len(result_df):\n",
    "        #    print(\"Scholar link:\", res.link)\n",
    "        #else:\n",
    "        #    print(\"{}\\n\" .format(res.link))\n",
    "        print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, dat=None):\n",
    "    result = search_googleish(query)\n",
    "    result = connect_id_df(result, meta)\n",
    "    result = rank_results(query, result)\n",
    "    print_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search for:machine\n",
      "*******************\n",
      "Kevin Warwick\n",
      "research field:  Biomedical Engineering, Machine Intelligence, Bioethics, Cybernetics, Cyborgs\n",
      "Research Interests: Cyborgs; Control\n",
      "; Robotics\n",
      "; Biomedical systems \n",
      "; Artificial intelligence\n",
      "Subfaculty:  Faculty Research Centre for Data Science\n",
      "Faculty:  Faculty Research Centre for Data Science\n",
      "CU link:  https://pureportal.coventry.ac.uk/en/persons/kevin-warwick\n",
      "GScolar link https://scholar.google.co.uk/citations?hl=en&user=TfTyMZQAAAAJ\n",
      "------------------------------------\n",
      "Vasile Palade\n",
      "research field:  Machine Learning, Data Science\n",
      "Research Interests: Machine Learning and Applications; Deep learning; Image Processing\n",
      "Subfaculty:  Faculty Research Centre for Data Science\n",
      "Faculty:  Faculty Research Centre for Data Science\n",
      "CU link:  https://pureportal.coventry.ac.uk/en/persons/vasile-palade\n",
      "GScolar link https://scholar.google.co.uk/citations?hl=en&user=KTXoxysAAAAJ\n",
      "------------------------------------\n",
      "Chitta Saha\n",
      "research field:  Energy harvesting, Thermo-acoustic, Electrical machine\n",
      "Research Interests:  My current research is involved in energy harvesting and its application areas, renewable energy, Thermo-acoustic technology and measurement and sensor technology. My Ph.D was on \n",
      "Subfaculty:  School of Computing, Electronics and Maths\n",
      "Faculty:  School of Computing, Electronics and Maths\n",
      "CU link:  https://pureportal.coventry.ac.uk/en/persons/chitta-saha\n",
      "GScolar link https://scholar.google.co.uk/citations?hl=en&user=7hUuoogAAAAJ\n",
      "------------------------------------\n",
      "Sara Sharifzadeh\n",
      "research field:  Artificial Intelligence, Machine Learning, Image/Signal Processing, Multivariate analysis, Intelligent Automation\n",
      "Research Interests: Signal, image and video analysisMultivariate data analysisMachine learningArtificial Intelligence (AI)Spectral signal/image analysis3D point cloud data analysisRoboticsEmbedded sys\n",
      "Subfaculty:  Faculty Research Centre for Data Science\n",
      "Faculty:  Faculty Research Centre for Data Science\n",
      "CU link:  https://pureportal.coventry.ac.uk/en/persons/sara-sharifzadeh\n",
      "GScolar link https://scholar.google.co.uk/citations?hl=en&user=p3hFOZgAAAAJ\n",
      "------------------------------------\n",
      "Mark Elshaw\n",
      "research field:  Machine learning and robotics\n",
      "Research Interests: Speech recognition; Robot-human interaction; Grounding of speech and actions in robots; Biomimetic robotics and their applications; Emotional production and recognition in robots; \n",
      "Subfaculty:  School of Computing, Electronics and Maths\n",
      "Faculty:  School of Computing, Electronics and Maths\n",
      "CU link:  https://pureportal.coventry.ac.uk/en/persons/mark-elshaw\n",
      "GScolar link https://scholar.google.co.uk/citations?hl=en&user=wPywZB0AAAAJ\n",
      "------------------------------------\n",
      "Christo Panchev\n",
      "research field:  Cybersecurity: Penetration Testing, Intrusion Detection Systems, Intelligent Systems: Machine Learning, Neural Networks, Mathema\n",
      "None: None\n",
      "Subfaculty:  School of Computing, Electronics and Maths\n",
      "Faculty:  School of Computing, Electronics and Maths\n",
      "CU link:  https://pureportal.coventry.ac.uk/en/persons/christo-panchev\n",
      "GScolar link https://scholar.google.co.uk/citations?hl=en&user=VaBsGb0AAAAJ\n",
      "------------------------------------\n",
      "Victoria Bloom\n",
      "research field:  computer vision, machine learning, action recognition\n",
      "nan\n",
      "Subfaculty:  nan\n",
      "Faculty:  nan\n",
      "CU link:  nan\n",
      "GScolar link https://scholar.google.co.uk/citations?hl=en&user=Rzz6lRgAAAAJ\n",
      "------------------------------------\n",
      "James Shuttleworth\n",
      "research field:  Image analysis, machine vision, python, programming, cyber security\n",
      "None: None\n",
      "Subfaculty:  School of Computing, Electronics and Maths\n",
      "Faculty:  School of Computing, Electronics and Maths\n",
      "CU link:  https://pureportal.coventry.ac.uk/en/persons/james-shuttleworth\n",
      "GScolar link https://scholar.google.co.uk/citations?hl=en&user=EOy0saAAAAAJ\n",
      "------------------------------------\n",
      "Adekunle Shaibu Shonola\n",
      "research field:  AI, E-learning, Machine Learning, Big Data, Data Science\n",
      "nan\n",
      "Subfaculty:  nan\n",
      "Faculty:  nan\n",
      "CU link:  nan\n",
      "GScolar link https://scholar.google.co.uk/citations?hl=en&user=69bSJSYAAAAJ\n",
      "------------------------------------\n",
      "Jino Mathew\n",
      "research field:  machine learning, applications\n",
      "None: None\n",
      "Subfaculty:  nan\n",
      "Faculty:  nan\n",
      "CU link:  https://pureportal.coventry.ac.uk/en/persons/jino-mathew\n",
      "GScolar link https://scholar.google.co.uk/citations?hl=en&user=MZK-UX4AAAAJ\n",
      "------------------------------------\n",
      "Dr. Diana Hintea\n",
      "research field:  Machine Learning, Thermal Comfort, HVAC Control, Artificial Intelligence, Digital Forensics\n",
      "nan\n",
      "Subfaculty:  nan\n",
      "Faculty:  nan\n",
      "CU link:  nan\n",
      "GScolar link https://scholar.google.co.uk/citations?hl=en&user=I7VtI_oAAAAJ\n",
      "------------------------------------\n",
      "Kojo Sarfo Gyamfi\n",
      "research field:  Classical Machine Learning, Neural Networks, Wireless Sensor Networks, Reinforcement Learning\n",
      "nan\n",
      "Subfaculty:  nan\n",
      "Faculty:  nan\n",
      "CU link:  nan\n",
      "GScolar link https://scholar.google.co.uk/citations?hl=en&user=VCHlzrgAAAAJ\n",
      "------------------------------------\n",
      "Ibrahim Almakky\n",
      "research field:  Deep Learning, Machine Learning, Representation Learning\n",
      "nan\n",
      "Subfaculty:  nan\n",
      "Faculty:  nan\n",
      "CU link:  nan\n",
      "GScolar link https://scholar.google.co.uk/citations?hl=en&user=T9MTcK0AAAAJ\n",
      "------------------------------------\n",
      "Yordanka Karayaneva\n",
      "research field:  Artificial Intelligence, Machine Learning, Signal Processing, Computer Vision\n",
      "None: None\n",
      "Subfaculty:  Faculty Research Centre for Data Science\n",
      "Faculty:  Faculty Research Centre for Data Science\n",
      "CU link:  https://pureportal.coventry.ac.uk/en/persons/yordanka-karayaneva\n",
      "GScolar link https://scholar.google.co.uk/citations?hl=en&user=lSHNeTAAAAAJ\n",
      "------------------------------------\n",
      "Felix Batsch\n",
      "research field:  Autonomous Vehicles, Machine Learning, Automated Driving\n",
      "Research Interests: Felix' research focuses on developing a new methodology to test autonomous vehicles using comprehensive vehicle simulation in conjunction with supervised machine learning. He is de\n",
      "Subfaculty:  nan\n",
      "Faculty:  nan\n",
      "CU link:  https://pureportal.coventry.ac.uk/en/persons/felix-batsch\n",
      "GScolar link https://scholar.google.co.uk/citations?hl=en&user=w9BVkUsAAAAJ\n",
      "------------------------------------\n",
      "Philip Bell\n",
      "research field:  Machine Learning, Data Mining, Big Data, Natural Language, Text Sentiment Analyse\n",
      "nan\n",
      "Subfaculty:  nan\n",
      "Faculty:  nan\n",
      "CU link:  nan\n",
      "GScolar link https://scholar.google.co.uk/citations?hl=en&user=OxuGRAoAAAAJ\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Search for:\")\n",
    "print('*******************')\n",
    "search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
